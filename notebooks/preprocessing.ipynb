{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/telegram_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking for NaN values in the 'Message' column:\")\n",
    "nan_count = df['Message'].isnull().sum()\n",
    "print(f\"Number of NaN values in 'Message' column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Message'])\n",
    "\n",
    "# Print the shape of the dataset after dropping NaN values in the \"Message\" column\n",
    "print(f\"Dataset shape after dropping NaN values in 'Message' column: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_df=df['Message']\n",
    "message_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of your DataFrame\n",
    "# df = pd.DataFrame({'Message': ['üí•3pcs silicon brush spatulas...', 'üí•Mandoline Slicer...', ...]})\n",
    "\n",
    "# Define a function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\" \n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the function to the 'Message' column\n",
    "df['Message'] = df['Message'].apply(remove_emojis)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def label_message_utf8_with_birr(message):\n",
    "    # Split the message at the first occurrence of '\\n'\n",
    "    if '\\n' in message:\n",
    "        first_line, remaining_message = message.split('\\n', 1)\n",
    "    else:\n",
    "        first_line, remaining_message = message, \"\"\n",
    "    \n",
    "    labeled_tokens = []\n",
    "    \n",
    "    # Tokenize the first line\n",
    "    first_line_tokens = re.findall(r'\\S+', first_line)\n",
    "    \n",
    "    # Label the first token as B-PRODUCT and the rest as I-PRODUCT\n",
    "    if first_line_tokens:\n",
    "        labeled_tokens.append(f\"{first_line_tokens[0]} B-PRODUCT\")  # First token as B-PRODUCT\n",
    "        for token in first_line_tokens[1:]:\n",
    "            labeled_tokens.append(f\"{token} I-PRODUCT\")  # Remaining tokens as I-PRODUCT\n",
    "    \n",
    "    # Process the remaining message normally\n",
    "    if remaining_message:\n",
    "        lines = remaining_message.split('\\n')\n",
    "        for line in lines:\n",
    "            tokens = re.findall(r'\\S+', line)  # Tokenize each line while considering non-ASCII characters\n",
    "            \n",
    "            for token in tokens:\n",
    "                # Check if token is a price (e.g., 500 ETB, $100, or ·â•·à≠)\n",
    "                if re.match(r'^\\d{10,}$', token):\n",
    "                    labeled_tokens.append(f\"{token} O\")  # Label as O for \"other\" or outside of any entity\n",
    "                elif re.match(r'^\\d+(\\.\\d{1,2})?$', token) or 'ETB' in token or '·ãã·åã' in token or '$' in token or '·â•·à≠' in token:\n",
    "                    labeled_tokens.append(f\"{token} I-PRICE\")\n",
    "                # Check if token could be a location (e.g., cities or general location names)\n",
    "                elif any(loc in token for loc in ['Addis Ababa', '·àà·â°', '·àà·â°¬†·àò·ã≥·àÖ·äí·ãì·àà·àù', '·àò·åà·äì·äõ', '·â¶·àå', '·àú·ä≠·à≤·äÆ']):\n",
    "                    labeled_tokens.append(f\"{token} I-LOC\")\n",
    "                # Assume other tokens are part of a product name or general text\n",
    "                else:\n",
    "                    labeled_tokens.append(f\"{token} O\")\n",
    "    \n",
    "    return \"\\n\".join(labeled_tokens)\n",
    "\n",
    "# Apply the updated function to the non-null messages\n",
    "df['Labeled_Message'] = df['Message'].apply(label_message_utf8_with_birr)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated labeled dataset to a file in CoNLL format\n",
    "labeled_data_birr_path = 'labeled_telegram_product_price_location.txt-'\n",
    "with open(labeled_data_birr_path, 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write(f\"{row['Labeled_Message']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = {\n",
    "#     'kids': [\n",
    "#         'toy', 'children', 'kids', '·àò·å´·ãà·âª', 'play', 'games', 'fun', 'educational', \n",
    "#         'puzzle', 'doll', 'action figure', 'stuffed animal', 'arts and crafts', \n",
    "#         'books', 'outdoor toys', 'building blocks', 'baby', 'toddler', 'Baby',\n",
    "#         '·àò·å´·ãà·âª·ãé·âΩ'\n",
    "#     ],\n",
    "#     'men': [\n",
    "#         'men', 'grooming', 'shaving', 'beard', 'razor', 'aftershave', \n",
    "#         'scent', 'deodorant', 'grooming kit', 'haircut', 'fashion', 'suits', \n",
    "#         'wallet', 'watch', 'accessories', 'fitness', 'shoes', \n",
    "#         '·ä†·àµ·â∞·ä´·ä≠·àç', '·ã®·â•·à≠·àÉ·äï ·ãï·âÉ·ãé·âΩ'\n",
    "#     ],\n",
    "#     'women': [\n",
    "#         'women', 'makeup', 'hair dryer', 'lipstick', 'foundation', 'mascara', \n",
    "#         'skincare', 'nails', 'jewelry', 'dresses', 'handbags', 'accessories', \n",
    "#         'fashion', 'shoes', 'perfume', 'hairstyle', 'wellness', 'beauty', 'style','Hair Drye',\n",
    "#         '·ä•·äï·âÖ·àµ·âÉ·à¥', '·ã®·çÄ·åâ·à≠ ·ä•·âÉ·ãé·âΩ', '·ã®·ãç·â†·âµ ·ä•·âÉ·ãé·âΩ'\n",
    "#     ],\n",
    "#     'sport': [\n",
    "#         'gym', 'GYM','fitness', 'exercise', '·ä•·äï·âÖ·àµ·âÉ·à¥', 'workout', 'training', 'yoga', \n",
    "#         'running', 'cycling', 'sportswear', 'equipment', 'weights', 'cardio', \n",
    "#         'aerobics', 'team sports', 'outdoor activities', 'athletics', 'health',  'workout', 'sports',\n",
    "#         '·àµ·çñ·à≠·âµ', '·ã®·ä•·äï·âÖ·àµ·âÉ·à¥ ·àò·à≥·à™·ã´·ãé·âΩ'\n",
    "#     ],\n",
    "#     'groceries': [\n",
    "#         'food', 'snacks', 'grocery', '·àù·åç·â•', 'produce', 'fruits', 'vegetables', \n",
    "#         'meat', 'dairy', 'bread', 'cereal', 'beverages', 'frozen', 'canned', \n",
    "#         'organic', 'bulk', 'condiments', 'spices', 'snack bars', 'breakfast', \n",
    "#         '·ä•·äï·âÅ·àã·àç', '·ãà·â∞·à≠', '·ã®·àù·åç·â• ·ä•·âÉ·ãé·âΩ'\n",
    "#     ],\n",
    "#     'accessories': [\n",
    "#         'jewelry', 'bags', 'accessory', '·âÄ·àà·â†·âµ', 'belts', 'hats', 'scarves', \n",
    "#         'sunglasses', 'watches', 'hair accessories', 'wallets', 'phone cases', \n",
    "#         'keychains', 'pins', 'brooches', 'fashion', 'style', 'gifts', 'decor', '·ã®·àç·â•·àµ ·àò·â∂·ä®·àª\\n\\n',\n",
    "#         '·ã®·àò·àç·ä≠·ãï ·ä•·âÉ·ãé·âΩ', '·ã®·àù·â≥·ãà·âÖ ·ä•·âÉ·ãé·âΩ','Anti-theft ',' Earbuds','PowerBank','Grip Tape','humidifier'\n",
    "#     ],\n",
    "#     'health': [\n",
    "#         'health', '·å§·äì', 'wellness', 'nutrition', 'vitamins', 'supplements', \n",
    "#         'exercise', 'fitness', 'mental health', 'meditation', 'stress relief', \n",
    "#         'doctor', 'check-up', 'first aid', 'hygiene', 'immune system', 'balance', \n",
    "#         'self-care', '·ä†·äï·ã∞·äõ ·å§·äì', '·ã®·å§·äì ·ä•·âÉ·ãé·âΩ','pulse'\n",
    "#     ],\n",
    "#     'household': [\n",
    "#         'cleaning', 'furniture', 'decor', 'appliances', 'utensils', 'kitchen', \n",
    "#         'bathroom', 'laundry', 'storage', 'organization', 'home improvement', 'pan', \n",
    "#         'gardening', 'tools', 'supplies', 'safety', 'maintenance', 'pets', 'spatulas','Kitchen','Mop',\n",
    "#         'spatulas\\n\\n','n·àà·ä™·âΩ·äï·ãé','home', 'comfort', '·â§·âµ', '·ã®·â§·âµ ·ä•·âÉ·ãé·âΩ', '·ä•·äï·âÖ·àµ·âÉ·à¥','bottle','·çî·à≠·àô·àµ','knife',\n",
    "#         'Glass','·ã®·àã·ãõ·äõ','stove','Ironing Board','Slicer','BLENDER','MULTIFUNCTIONAL BLENDER','Toilet Brush',\n",
    "#         '·ã®·â¢·àã ·àµ·â•·àµ·â•','·â¢·àã','Oven','fridge', '·àò·å•·â†·àª','Toilet','Mob','cookware','Blender','KITCHENWARE','·àù·äï·å£·çç','Tablemats'\n",
    "#     ]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_message_utf8_with_birr(message):\n",
    "    tokens = re.findall(r'\\S+', message)  # Tokenize while considering non-ASCII characters\n",
    "    labeled_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Check if token is a price (e.g., 500 ETB, $100, or ·â•·à≠)\n",
    "        \n",
    "        if re.match(r'^\\d{10,}$', token):\n",
    "            labeled_tokens.append(f\"{token} O\")  # Label as O for \"other\" or outside of any entity\n",
    "        elif re.match(r'^\\d+(\\.\\d{1,2})?$', token) or 'ETB' in token or '$' in token or '·â•·à≠' in token:\n",
    "            labeled_tokens.append(f\"{token} I-PRICE\")\n",
    "        \n",
    "        # Check if token could be a location (e.g., cities or general location names)\n",
    "        elif any(loc in token for loc in ['Addis Ababa', '·àà·â°', '·àà·â°¬† ·àò·ã≥·àÖ·äí·ãì·àà·àù', '·àò·åà·äì·äõ','·â¶·àå','·àú·ä≠·à≤·äÆ']):\n",
    "            labeled_tokens.append(f\"{token} I-LOC\")\n",
    "        \n",
    "        elif any(loc in token for loc in ['üí•']):\n",
    "            labeled_tokens.append(f\"{token} B-Product\")\n",
    "        \n",
    "        # Assume other tokens are part of a product name (this can be refined)\n",
    "        else:\n",
    "            labeled_tokens.append(f\"{token} O\")\n",
    "    \n",
    "    return \"\\n\".join(labeled_tokens)\n",
    "\n",
    "# Apply the updated function to the non-null messages\n",
    "df['Labeled_Message'] = df['Message'].apply(label_message_utf8_with_birr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated labeled dataset to a file in CoNLL format\n",
    "labeled_data_birr_path = 'labeled_telegram_data_price_product_location_birr.txt'\n",
    "with open(labeled_data_birr_path, 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write(f\"{row['Labeled_Message']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = df['Message'][10]\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message'][10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a string contains Amharic characters\n",
    "def is_amharic(message):\n",
    "    return bool(re.search(r'[\\u1200-\\u137F]', message))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify messages\n",
    "def classify_message(message):\n",
    "    if pd.isna(message):  # Check for NaN or None\n",
    "        return 'uncategorized'\n",
    "    \n",
    "    if is_amharic(message):\n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in message for keyword in keywords):\n",
    "                return category\n",
    "    else:\n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in message.lower() for keyword in keywords):\n",
    "                return category\n",
    "    return 'uncategorized'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classification to the Message column\n",
    "df['Category'] = df['Message'].apply(classify_message)\n",
    "\n",
    "# Display the updated DataFrame with categories\n",
    "print(df[['Message', 'Category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display counts of unique values in the Category column\n",
    "category_counts = df['Category'].value_counts()\n",
    "print(category_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncategorized_items = df[df['Category'] == 'uncategorized']\n",
    "uncategorized_items.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncategorized_items.to_csv('uncategorized_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
